# Story 3.3: Migration Execution Engine

## Story

As a user,
I want a robust migration execution engine that safely executes my migration plans,
So that I can migrate large amounts of data with confidence, real-time progress tracking, error recovery, and the ability to pause, resume, or rollback migrations as needed.

## Story Context

**Epic:** 3 - Migration Planning and Execution  
**Dependencies:** Story 3.1 (Migration Planning Foundation), Story 3.2 (Advanced Organization Rules), Epic 2 Complete  
**Story Type:** Core Feature - Migration Execution  

This story implements the actual execution of migration plans created in Stories 3.1 and 3.2. It leverages the task engine from Epic 2 to perform complex migrations safely, with comprehensive error handling, progress tracking, and recovery capabilities. This is the culmination of MediaMogul's core value proposition.

## Acceptance Criteria

### Migration Execution Management
- [ ] **Plan execution:** Convert migration plans into executable task sequences
- [ ] **Execution modes:** Support dry-run, test migration, and full migration modes
- [ ] **Batch processing:** Execute migrations in configurable batches to manage resource usage
- [ ] **Dependency handling:** Ensure tasks execute in correct order respecting dependencies
- [ ] **Resource management:** Monitor and limit CPU, memory, and I/O usage during migration
- [ ] **Concurrent operations:** Support multiple migration plans executing simultaneously

### Safe Migration Operations
- [ ] **Pre-flight validation:** Comprehensive validation before migration starts
- [ ] **Space verification:** Ensure adequate space at destination before each batch
- [ ] **Permission checks:** Verify read/write permissions for all source and destination paths
- [ ] **Conflict resolution:** Handle filename conflicts according to migration strategy
- [ ] **Atomic operations:** Ensure individual file operations are atomic and recoverable
- [ ] **Verification:** Verify successful migration using checksums and file attributes

### Progress Tracking & Monitoring
- [ ] **Real-time progress:** Live updates on migration progress across all active plans
- [ ] **Detailed statistics:** Files processed, bytes transferred, speed, time remaining
- [ ] **Task breakdown:** Progress tracking for individual tasks within migration
- [ ] **Error tracking:** Comprehensive error logging with categorization and retry status
- [ ] **Performance metrics:** Transfer speeds, I/O patterns, resource utilization
- [ ] **Milestone reporting:** Key migration milestones and completion notifications

### Error Handling & Recovery
- [ ] **Error categorization:** Classify errors by type (transient, permanent, user action required)
- [ ] **Automatic retry:** Intelligent retry logic for transient errors with backoff
- [ ] **Error reporting:** Detailed error messages with suggested remediation steps
- [ ] **Partial failure handling:** Continue migration when individual files fail
- [ ] **Recovery mechanisms:** Resume interrupted migrations from last successful point
- [ ] **Rollback capability:** Ability to rollback completed operations if needed

### Migration Control Operations
- [ ] **Pause/Resume:** Ability to pause and resume migrations without data loss
- [ ] **Priority adjustment:** Change migration priority during execution
- [ ] **Selective execution:** Skip or retry specific files or batches
- [ ] **Emergency stop:** Immediate halt with safe cleanup of in-progress operations
- [ ] **Schedule management:** Support for scheduled migrations and timing controls
- [ ] **Resource throttling:** Dynamic adjustment of resource usage during execution

### Verification & Validation
- [ ] **Integrity verification:** MD5 checksum verification for all transferred files
- [ ] **Metadata preservation:** Preserve file timestamps, permissions, and attributes
- [ ] **Structure validation:** Verify destination directory structure matches plan
- [ ] **Completeness checking:** Ensure all planned files were successfully migrated
- [ ] **Duplicate validation:** Verify duplicate handling worked as specified
- [ ] **Post-migration reporting:** Comprehensive reports on migration results

## Technical Implementation

### Backend Components

**Migration Execution Engine** (`internal/services/migration_executor.go`)
```go
type MigrationExecutor struct {
    db              *database.DB
    taskService     *TaskService
    plannerService  *MigrationPlannerService
    orgEngine       *OrganizationEngine
    fileService     *FileService
    diskService     *DiskService
    logger          *slog.Logger
    activeExecutions sync.Map // map[string]*ExecutionContext
}

type ExecutionContext struct {
    PlanID          string                    `json:"plan_id"`
    ExecutionID     string                    `json:"execution_id"`
    Status          ExecutionStatus           `json:"status"`
    StartedAt       time.Time                 `json:"started_at"`
    UpdatedAt       time.Time                 `json:"updated_at"`
    CompletedAt     *time.Time                `json:"completed_at,omitempty"`
    Progress        ExecutionProgress         `json:"progress"`
    Tasks           []string                  `json:"tasks"` // Task UIDs
    Errors          []ExecutionError          `json:"errors"`
    Settings        ExecutionSettings         `json:"settings"`
    mutex           sync.RWMutex
}

type ExecutionStatus string
const (
    ExecutionStatusPending    ExecutionStatus = "pending"
    ExecutionStatusRunning    ExecutionStatus = "running"
    ExecutionStatusPaused     ExecutionStatus = "paused"
    ExecutionStatusCompleted  ExecutionStatus = "completed"
    ExecutionStatusFailed     ExecutionStatus = "failed"
    ExecutionStatusCancelled  ExecutionStatus = "cancelled"
)

type ExecutionProgress struct {
    TotalFiles         int64   `json:"total_files"`
    ProcessedFiles     int64   `json:"processed_files"`
    SuccessfulFiles    int64   `json:"successful_files"`
    FailedFiles        int64   `json:"failed_files"`
    SkippedFiles       int64   `json:"skipped_files"`
    TotalBytes         int64   `json:"total_bytes"`
    ProcessedBytes     int64   `json:"processed_bytes"`
    TransferredBytes   int64   `json:"transferred_bytes"`
    CurrentBatch       int     `json:"current_batch"`
    TotalBatches       int     `json:"total_batches"`
    EstimatedTimeRemaining int64 `json:"estimated_time_remaining"`
    TransferSpeed      int64   `json:"transfer_speed"`
    PercentComplete    float64 `json:"percent_complete"`
}

type ExecutionSettings struct {
    Mode              ExecutionMode     `json:"mode"`
    BatchSize         int               `json:"batch_size"`
    MaxConcurrentTasks int              `json:"max_concurrent_tasks"`
    ResourceLimits    ResourceLimits    `json:"resource_limits"`
    RetryPolicy       RetryPolicy       `json:"retry_policy"`
    VerificationLevel VerificationLevel `json:"verification_level"`
    CleanupPolicy     CleanupPolicy     `json:"cleanup_policy"`
}

func NewMigrationExecutor(deps ServiceDependencies) *MigrationExecutor {
    return &MigrationExecutor{
        db:             deps.DB,
        taskService:    deps.TaskService,
        plannerService: deps.PlannerService,
        orgEngine:     deps.OrganizationEngine,
        fileService:   deps.FileService,
        diskService:   deps.DiskService,
        logger:        deps.Logger.With("service", "migration_executor"),
    }
}

// ExecutePlan starts execution of a migration plan
func (e *MigrationExecutor) ExecutePlan(ctx context.Context, planID string, settings ExecutionSettings) (*ExecutionContext, error) {
    plan, err := e.plannerService.GetPlan(ctx, planID)
    if err != nil {
        return nil, fmt.Errorf("failed to get migration plan: %w", err)
    }
    
    if plan.Status != PlanStatusReady {
        return nil, fmt.Errorf("plan must be in ready status, current status: %s", plan.Status)
    }
    
    // Create execution context
    execCtx := &ExecutionContext{
        PlanID:      planID,
        ExecutionID: generateExecutionID(),
        Status:      ExecutionStatusPending,
        StartedAt:   time.Now(),
        UpdatedAt:   time.Now(),
        Progress:    ExecutionProgress{},
        Settings:    settings,
        Tasks:       []string{},
        Errors:      []ExecutionError{},
    }
    
    // Store execution context
    e.activeExecutions.Store(execCtx.ExecutionID, execCtx)
    
    // Perform pre-flight validation
    if err := e.preFlightValidation(ctx, plan, execCtx); err != nil {
        execCtx.Status = ExecutionStatusFailed
        execCtx.Errors = append(execCtx.Errors, ExecutionError{
            Type:        ErrorTypeValidation,
            Message:     fmt.Sprintf("Pre-flight validation failed: %v", err),
            Timestamp:   time.Now(),
            Recoverable: false,
        })
        return execCtx, fmt.Errorf("pre-flight validation failed: %w", err)
    }
    
    // Generate migration tasks
    tasks, err := e.generateMigrationTasks(ctx, plan, execCtx)
    if err != nil {
        execCtx.Status = ExecutionStatusFailed
        return execCtx, fmt.Errorf("failed to generate migration tasks: %w", err)
    }
    
    // Initialize progress tracking
    e.initializeProgress(ctx, execCtx, tasks)
    
    // Start execution in background
    go e.executeMigration(ctx, plan, execCtx, tasks)
    
    // Update plan status
    e.plannerService.UpdatePlanStatus(ctx, planID, PlanStatusExecuting)
    
    e.logger.Info("Migration execution started", "plan_id", planID, "execution_id", execCtx.ExecutionID, "tasks", len(tasks))
    return execCtx, nil
}

// executeMigration performs the actual migration execution
func (e *MigrationExecutor) executeMigration(ctx context.Context, plan *MigrationPlan, execCtx *ExecutionContext, tasks []Task) {
    execCtx.mutex.Lock()
    execCtx.Status = ExecutionStatusRunning
    execCtx.UpdatedAt = time.Now()
    execCtx.mutex.Unlock()
    
    defer func() {
        if r := recover(); r != nil {
            e.logger.Error("Migration execution panic", "execution_id", execCtx.ExecutionID, "panic", r)
            execCtx.mutex.Lock()
            execCtx.Status = ExecutionStatusFailed
            execCtx.Errors = append(execCtx.Errors, ExecutionError{
                Type:        ErrorTypeSystem,
                Message:     fmt.Sprintf("System error during execution: %v", r),
                Timestamp:   time.Now(),
                Recoverable: false,
            })
            execCtx.UpdatedAt = time.Now()
            execCtx.mutex.Unlock()
        }
    }()
    
    // Execute tasks in batches
    batches := e.createTaskBatches(tasks, execCtx.Settings.BatchSize)
    execCtx.Progress.TotalBatches = len(batches)
    
    for batchIndex, batch := range batches {
        // Check if execution should continue
        if e.shouldStopExecution(execCtx) {
            break
        }
        
        execCtx.mutex.Lock()
        execCtx.Progress.CurrentBatch = batchIndex + 1
        execCtx.UpdatedAt = time.Now()
        execCtx.mutex.Unlock()
        
        // Execute batch
        if err := e.executeBatch(ctx, batch, execCtx); err != nil {
            e.logger.Error("Batch execution failed", "execution_id", execCtx.ExecutionID, "batch", batchIndex, "error", err)
            
            // Determine if we should continue or stop
            if e.shouldContinueAfterBatchError(err, execCtx.Settings) {
                e.recordBatchError(execCtx, batchIndex, err)
                continue
            } else {
                execCtx.mutex.Lock()
                execCtx.Status = ExecutionStatusFailed
                execCtx.UpdatedAt = time.Now()
                execCtx.mutex.Unlock()
                break
            }
        }
        
        // Update progress after successful batch
        e.updateProgressAfterBatch(execCtx, batch)
    }
    
    // Finalize execution
    e.finalizeExecution(ctx, plan, execCtx)
}

func (e *MigrationExecutor) executeBatch(ctx context.Context, batch []Task, execCtx *ExecutionContext) error {
    // Submit all tasks in the batch
    var taskUIDs []string
    for _, task := range batch {
        if err := e.taskService.SubmitTask(ctx, task); err != nil {
            return fmt.Errorf("failed to submit task %s: %w", task.UID, err)
        }
        taskUIDs = append(taskUIDs, task.UID)
    }
    
    // Store task UIDs for tracking
    execCtx.mutex.Lock()
    execCtx.Tasks = append(execCtx.Tasks, taskUIDs...)
    execCtx.mutex.Unlock()
    
    // Monitor batch execution
    return e.monitorBatchExecution(ctx, taskUIDs, execCtx)
}

func (e *MigrationExecutor) monitorBatchExecution(ctx context.Context, taskUIDs []string, execCtx *ExecutionContext) error {
    ticker := time.NewTicker(1 * time.Second)
    defer ticker.Stop()
    
    completedTasks := make(map[string]bool)
    
    for {
        select {
        case <-ctx.Done():
            return ctx.Err()
        case <-ticker.C:
            allComplete := true
            var batchErrors []error
            
            for _, taskUID := range taskUIDs {
                if completedTasks[taskUID] {
                    continue
                }
                
                task, err := e.taskService.GetTask(ctx, taskUID)
                if err != nil {
                    e.logger.Error("Failed to get task status", "task_id", taskUID, "error", err)
                    continue
                }
                
                switch task.Status {
                case "complete":
                    completedTasks[taskUID] = true
                    e.updateProgressForTask(execCtx, task, true)
                    
                case "failed":
                    completedTasks[taskUID] = true
                    e.updateProgressForTask(execCtx, task, false)
                    batchErrors = append(batchErrors, fmt.Errorf("task %s failed: %s", taskUID, task.ErrorMessage))
                    
                case "cancelled":
                    completedTasks[taskUID] = true
                    e.updateProgressForTask(execCtx, task, false)
                    
                default:
                    allComplete = false
                    // Update progress for running tasks
                    e.updateProgressForTask(execCtx, task, false)
                }
            }
            
            if allComplete {
                if len(batchErrors) > 0 {
                    return fmt.Errorf("batch completed with errors: %v", batchErrors)
                }
                return nil
            }
            
            // Check if execution should be paused or stopped
            if e.shouldStopExecution(execCtx) {
                return fmt.Errorf("execution stopped by user")
            }
        }
    }
}

// generateMigrationTasks creates tasks from migration plan
func (e *MigrationExecutor) generateMigrationTasks(ctx context.Context, plan *MigrationPlan, execCtx *ExecutionContext) ([]Task, error) {
    var tasks []Task
    
    // Get files to migrate based on source configuration
    files, err := e.fileService.FindFiles(ctx, FileSearchCriteria{
        Disks:           plan.SourceConfig.Disks,
        Paths:           plan.SourceConfig.Paths,
        IncludePatterns: plan.SourceConfig.IncludePatterns,
        ExcludePatterns: plan.SourceConfig.ExcludePatterns,
        MinSize:         plan.SourceConfig.MinFileSize,
        MaxSize:         plan.SourceConfig.MaxFileSize,
        FileTypes:       plan.SourceConfig.FileTypes,
    })
    if err != nil {
        return nil, fmt.Errorf("failed to find files: %w", err)
    }
    
    // Apply organization rules to files
    organizedFiles, err := e.orgEngine.ApplyRules(ctx, files, plan.Strategy.OrganizationRules)
    if err != nil {
        return nil, fmt.Errorf("failed to apply organization rules: %w", err)
    }
    
    // Group files by target directory for efficient batch operations
    dirGroups := e.groupFilesByDirectory(organizedFiles)
    
    // Create directory creation tasks first
    for targetDir := range dirGroups {
        tasks = append(tasks, Task{
            UID:               generateTaskUID(),
            TaskType:          "create_directory",
            ExecutionPriority: "immediate",
            Description:       fmt.Sprintf("Create directory: %s", targetDir),
            Parameters: map[string]interface{}{
                "target_directory": targetDir,
                "create_parents":   true,
            },
        })
    }
    
    // Create file movement/copy tasks
    for targetDir, dirFiles := range dirGroups {
        // Group files into batches for the directory
        batches := e.batchFilesBySize(dirFiles, plan.Strategy.BatchSize)
        
        for batchIndex, batch := range batches {
            var fileOperations []FileOperation
            
            for _, orgFile := range batch {
                operation := FileOperation{
                    SourcePath:      orgFile.File.Path,
                    DestinationPath: orgFile.TargetPath,
                    Operation:       e.determineFileOperation(plan.Strategy, orgFile),
                    VerifyChecksum:  true,
                    PreserveMetadata: true,
                }
                fileOperations = append(fileOperations, operation)
            }
            
            task := Task{
                UID:               generateTaskUID(),
                TaskType:          "batch_file_operation",
                ExecutionPriority: e.determineTaskPriority(plan.Strategy, len(fileOperations)),
                Description:       fmt.Sprintf("Migrate %d files to %s (batch %d)", len(fileOperations), targetDir, batchIndex+1),
                Parameters: map[string]interface{}{
                    "operations":       fileOperations,
                    "target_directory": targetDir,
                    "conflict_resolution": plan.Strategy.ConflictResolution,
                    "verify_integrity": execCtx.Settings.VerificationLevel != VerificationLevelNone,
                },
            }
            
            tasks = append(tasks, task)
        }
    }
    
    // Create verification tasks if required
    if execCtx.Settings.VerificationLevel == VerificationLevelFull {
        verificationTask := Task{
            UID:               generateTaskUID(),
            TaskType:          "migration_verification",
            ExecutionPriority: "scheduled",
            Description:       "Verify migration completeness and integrity",
            Parameters: map[string]interface{}{
                "plan_id":       plan.ID,
                "execution_id":  execCtx.ExecutionID,
                "verify_checksums": true,
                "verify_structure": true,
                "verify_metadata":  true,
            },
        }
        tasks = append(tasks, verificationTask)
    }
    
    return tasks, nil
}

// preFlightValidation performs comprehensive validation before execution
func (e *MigrationExecutor) preFlightValidation(ctx context.Context, plan *MigrationPlan, execCtx *ExecutionContext) error {
    var validationErrors []error
    
    // Validate source accessibility
    for _, disk := range plan.SourceConfig.Disks {
        if !e.diskService.IsDiskAccessible(ctx, disk) {
            validationErrors = append(validationErrors, fmt.Errorf("source disk %s is not accessible", disk))
        }
    }
    
    // Validate destination accessibility and space
    targetDisk := plan.DestConfig.TargetDisk
    if !e.diskService.IsDiskAccessible(ctx, targetDisk) {
        validationErrors = append(validationErrors, fmt.Errorf("destination disk %s is not accessible", targetDisk))
    }
    
    // Check available space
    requiredSpace := plan.Analysis.SpaceAnalysis.TotalSize
    availableSpace, err := e.diskService.GetAvailableSpace(ctx, targetDisk)
    if err != nil {
        validationErrors = append(validationErrors, fmt.Errorf("failed to check available space on %s: %w", targetDisk, err))
    } else if availableSpace < requiredSpace+plan.DestConfig.SpaceReservation {
        validationErrors = append(validationErrors, fmt.Errorf("insufficient space on destination disk: required %d, available %d", 
            requiredSpace+plan.DestConfig.SpaceReservation, availableSpace))
    }
    
    // Validate permissions
    if !e.hasWritePermission(plan.DestConfig.BasePath) {
        validationErrors = append(validationErrors, fmt.Errorf("no write permission to destination path: %s", plan.DestConfig.BasePath))
    }
    
    // Check for critical conflicts
    if len(plan.Analysis.RiskAnalysis) > 0 {
        for _, risk := range plan.Analysis.RiskAnalysis {
            if risk.Severity == "critical" && !risk.CanContinue {
                validationErrors = append(validationErrors, fmt.Errorf("critical risk prevents migration: %s", risk.Description))
            }
        }
    }
    
    if len(validationErrors) > 0 {
        return fmt.Errorf("validation failed with %d errors: %v", len(validationErrors), validationErrors)
    }
    
    return nil
}
```

**Task Implementations** (`internal/tasks/migration_tasks.go`)
```go
// BatchFileOperationTask handles batch file operations for migration
type BatchFileOperationTask struct {
    taskBase
    fileService    *FileService
    checksumService *ChecksumService
}

func (t *BatchFileOperationTask) Execute(ctx context.Context, params map[string]interface{}) error {
    operations, ok := params["operations"].([]FileOperation)
    if !ok {
        return fmt.Errorf("invalid operations parameter")
    }
    
    targetDir, ok := params["target_directory"].(string)
    if !ok {
        return fmt.Errorf("invalid target_directory parameter")
    }
    
    verifyIntegrity, _ := params["verify_integrity"].(bool)
    
    // Ensure target directory exists
    if err := os.MkdirAll(targetDir, 0755); err != nil {
        return fmt.Errorf("failed to create target directory %s: %w", targetDir, err)
    }
    
    var successCount, failureCount int
    var totalBytes int64
    
    for i, operation := range operations {
        // Update progress
        t.UpdateProgress(float64(i) / float64(len(operations)) * 100)
        
        // Perform the file operation
        err := t.performFileOperation(ctx, operation, verifyIntegrity)
        if err != nil {
            t.logger.Error("File operation failed", "source", operation.SourcePath, "dest", operation.DestinationPath, "error", err)
            failureCount++
            // Continue with other files unless it's a critical error
            continue
        }
        
        successCount++
        
        // Get file size for progress tracking
        if stat, err := os.Stat(operation.DestinationPath); err == nil {
            totalBytes += stat.Size()
            t.UpdateBytesProcessed(totalBytes)
        }
    }
    
    t.logger.Info("Batch file operation completed", 
        "total", len(operations), 
        "success", successCount, 
        "failures", failureCount,
        "bytes", totalBytes)
    
    if failureCount > 0 && successCount == 0 {
        return fmt.Errorf("all file operations failed")
    }
    
    return nil
}

func (t *BatchFileOperationTask) performFileOperation(ctx context.Context, op FileOperation, verifyIntegrity bool) error {
    switch op.Operation {
    case "copy":
        return t.copyFile(ctx, op, verifyIntegrity)
    case "move":
        return t.moveFile(ctx, op, verifyIntegrity)
    case "hardlink":
        return t.hardlinkFile(ctx, op)
    default:
        return fmt.Errorf("unknown operation: %s", op.Operation)
    }
}

func (t *BatchFileOperationTask) copyFile(ctx context.Context, op FileOperation, verifyIntegrity bool) error {
    // Open source file
    src, err := os.Open(op.SourcePath)
    if err != nil {
        return fmt.Errorf("failed to open source file: %w", err)
    }
    defer src.Close()
    
    // Create destination file
    dst, err := os.Create(op.DestinationPath)
    if err != nil {
        return fmt.Errorf("failed to create destination file: %w", err)
    }
    defer dst.Close()
    
    // Copy with progress tracking
    var written int64
    if verifyIntegrity {
        // Copy with checksum verification
        written, err = t.copyWithVerification(ctx, src, dst, op)
    } else {
        // Simple copy
        written, err = io.Copy(dst, src)
    }
    
    if err != nil {
        os.Remove(op.DestinationPath) // Cleanup on failure
        return fmt.Errorf("copy operation failed: %w", err)
    }
    
    // Preserve metadata if requested
    if op.PreserveMetadata {
        if err := t.preserveFileMetadata(op.SourcePath, op.DestinationPath); err != nil {
            t.logger.Warn("Failed to preserve metadata", "source", op.SourcePath, "dest", op.DestinationPath, "error", err)
        }
    }
    
    t.logger.Debug("File copied successfully", "source", op.SourcePath, "dest", op.DestinationPath, "bytes", written)
    return nil
}

func (t *BatchFileOperationTask) copyWithVerification(ctx context.Context, src, dst io.ReadWriter, op FileOperation) (int64, error) {
    // Create hash writers for both source and destination
    srcHash := sha256.New()
    dstHash := sha256.New()
    
    // Create multi-writers
    srcReader := io.TeeReader(src, srcHash)
    dstWriter := io.MultiWriter(dst, dstHash)
    
    // Perform copy
    written, err := io.Copy(dstWriter, srcReader)
    if err != nil {
        return written, err
    }
    
    // Verify checksums match
    srcSum := srcHash.Sum(nil)
    dstSum := dstHash.Sum(nil)
    
    if !bytes.Equal(srcSum, dstSum) {
        return written, fmt.Errorf("checksum verification failed: source %x, destination %x", srcSum, dstSum)
    }
    
    return written, nil
}

// MigrationVerificationTask verifies migration completeness
type MigrationVerificationTask struct {
    taskBase
    executor       *MigrationExecutor
    fileService    *FileService
    checksumService *ChecksumService
}

func (t *MigrationVerificationTask) Execute(ctx context.Context, params map[string]interface{}) error {
    planID, ok := params["plan_id"].(string)
    if !ok {
        return fmt.Errorf("invalid plan_id parameter")
    }
    
    executionID, ok := params["execution_id"].(string)
    if !ok {
        return fmt.Errorf("invalid execution_id parameter")
    }
    
    verifyChecksums, _ := params["verify_checksums"].(bool)
    verifyStructure, _ := params["verify_structure"].(bool)
    verifyMetadata, _ := params["verify_metadata"].(bool)
    
    // Get migration plan and execution context
    plan, err := t.executor.plannerService.GetPlan(ctx, planID)
    if err != nil {
        return fmt.Errorf("failed to get migration plan: %w", err)
    }
    
    execCtx, err := t.executor.GetExecutionContext(executionID)
    if err != nil {
        return fmt.Errorf("failed to get execution context: %w", err)
    }
    
    var verificationErrors []error
    var verifiedFiles int
    
    // Verify file integrity
    if verifyChecksums {
        checksumErrors := t.verifyFileChecksums(ctx, plan, execCtx)
        verificationErrors = append(verificationErrors, checksumErrors...)
    }
    
    // Verify directory structure
    if verifyStructure {
        structureErrors := t.verifyDirectoryStructure(ctx, plan)
        verificationErrors = append(verificationErrors, structureErrors...)
    }
    
    // Verify metadata preservation
    if verifyMetadata {
        metadataErrors := t.verifyFileMetadata(ctx, plan, execCtx)
        verificationErrors = append(verificationErrors, metadataErrors...)
    }
    
    // Update verification results
    verificationResult := VerificationResult{
        PlanID:            planID,
        ExecutionID:       executionID,
        VerifiedFiles:     verifiedFiles,
        ErrorCount:        len(verificationErrors),
        Errors:           verificationErrors,
        VerificationTime:  time.Now(),
    }
    
    // Store verification results
    if err := t.executor.StoreVerificationResult(ctx, verificationResult); err != nil {
        t.logger.Error("Failed to store verification results", "error", err)
    }
    
    if len(verificationErrors) > 0 {
        return fmt.Errorf("verification failed with %d errors", len(verificationErrors))
    }
    
    t.logger.Info("Migration verification completed successfully", "plan_id", planID, "verified_files", verifiedFiles)
    return nil
}
```

### API Endpoints

**Migration Execution API** (`internal/api/migration_execution.go`)
```go
// POST /api/v1/migration-plans/:id/execute
func (h *MigrationExecutionHandler) ExecutePlan(c *gin.Context) {
    planID := c.Param("id")
    
    var req ExecutePlanRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(400, gin.H{"error": "Invalid request body", "details": err.Error()})
        return
    }
    
    execCtx, err := h.executor.ExecutePlan(c.Request.Context(), planID, req.Settings)
    if err != nil {
        h.logger.Error("Failed to execute migration plan", "plan_id", planID, "error", err)
        c.JSON(500, gin.H{"error": "Failed to execute migration plan", "details": err.Error()})
        return
    }
    
    c.JSON(202, gin.H{
        "message": "Migration execution started",
        "execution_id": execCtx.ExecutionID,
        "status": execCtx.Status,
    })
}

// GET /api/v1/migration-executions/:id
func (h *MigrationExecutionHandler) GetExecution(c *gin.Context) {
    executionID := c.Param("id")
    
    execCtx, err := h.executor.GetExecutionContext(executionID)
    if err != nil {
        if errors.Is(err, ErrExecutionNotFound) {
            c.JSON(404, gin.H{"error": "Execution not found"})
            return
        }
        h.logger.Error("Failed to get execution context", "execution_id", executionID, "error", err)
        c.JSON(500, gin.H{"error": "Failed to get execution"})
        return
    }
    
    c.JSON(200, execCtx)
}

// POST /api/v1/migration-executions/:id/pause
func (h *MigrationExecutionHandler) PauseExecution(c *gin.Context) {
    executionID := c.Param("id")
    
    err := h.executor.PauseExecution(c.Request.Context(), executionID)
    if err != nil {
        h.logger.Error("Failed to pause execution", "execution_id", executionID, "error", err)
        c.JSON(500, gin.H{"error": "Failed to pause execution"})
        return
    }
    
    c.JSON(200, gin.H{"message": "Execution paused", "execution_id": executionID})
}

// POST /api/v1/migration-executions/:id/resume
func (h *MigrationExecutionHandler) ResumeExecution(c *gin.Context) {
    executionID := c.Param("id")
    
    err := h.executor.ResumeExecution(c.Request.Context(), executionID)
    if err != nil {
        h.logger.Error("Failed to resume execution", "execution_id", executionID, "error", err)
        c.JSON(500, gin.H{"error": "Failed to resume execution"})
        return
    }
    
    c.JSON(200, gin.H{"message": "Execution resumed", "execution_id": executionID})
}

// POST /api/v1/migration-executions/:id/cancel
func (h *MigrationExecutionHandler) CancelExecution(c *gin.Context) {
    executionID := c.Param("id")
    
    err := h.executor.CancelExecution(c.Request.Context(), executionID)
    if err != nil {
        h.logger.Error("Failed to cancel execution", "execution_id", executionID, "error", err)
        c.JSON(500, gin.H{"error": "Failed to cancel execution"})
        return
    }
    
    c.JSON(200, gin.H{"message": "Execution cancelled", "execution_id": executionID})
}

// GET /api/v1/migration-executions
func (h *MigrationExecutionHandler) ListExecutions(c *gin.Context) {
    status := c.Query("status")
    planID := c.Query("plan_id")
    
    executions, err := h.executor.ListExecutions(c.Request.Context(), ListExecutionsFilter{
        Status: status,
        PlanID: planID,
    })
    if err != nil {
        h.logger.Error("Failed to list executions", "error", err)
        c.JSON(500, gin.H{"error": "Failed to list executions"})
        return
    }
    
    c.JSON(200, gin.H{"executions": executions})
}
```

### Frontend Components

**Migration Execution Monitor** (`app/migration/execution/[id]/page.tsx`)
```tsx
'use client'
export default function MigrationExecutionPage({ params }: { params: { id: string } }) {
  const { execution, isLoading, pauseExecution, resumeExecution, cancelExecution } = useMigrationExecution(params.id)
  
  if (isLoading) {
    return <ExecutionLoadingSkeleton />
  }
  
  if (!execution) {
    return <ExecutionNotFound />
  }
  
  return (
    <div className="migration-execution-container">
      <ExecutionHeader 
        execution={execution}
        onPause={pauseExecution}
        onResume={resumeExecution}
        onCancel={cancelExecution}
      />
      
      <div className="execution-content">
        <div className="execution-main">
          <ExecutionProgress execution={execution} />
          <ExecutionTasks execution={execution} />
          <ExecutionErrors execution={execution} />
        </div>
        
        <div className="execution-sidebar">
          <ExecutionStats execution={execution} />
          <ExecutionSettings execution={execution} />
        </div>
      </div>
    </div>
  )
}
```

**Execution Progress Component** (`components/migration/ExecutionProgress.tsx`)
```tsx
export function ExecutionProgress({ execution }: { execution: ExecutionContext }) {
  const progress = execution.progress
  
  return (
    <div className="execution-progress">
      <div className="progress-header">
        <h2>Migration Progress</h2>
        <ExecutionStatusBadge status={execution.status} />
      </div>
      
      <div className="progress-overview">
        <div className="progress-bar-container">
          <Progress 
            value={progress.percentComplete} 
            className="main-progress-bar"
          />
          <span className="progress-text">
            {progress.percentComplete.toFixed(1)}% Complete
          </span>
        </div>
        
        <div className="progress-stats-grid">
          <StatCard 
            title="Files Processed" 
            value={`${progress.processedFiles.toLocaleString()} / ${progress.totalFiles.toLocaleString()}`}
            icon={FileIcon}
          />
          <StatCard 
            title="Data Transferred" 
            value={`${formatBytes(progress.transferredBytes)} / ${formatBytes(progress.totalBytes)}`}
            icon={HardDriveIcon}
          />
          <StatCard 
            title="Transfer Speed" 
            value={`${formatBytes(progress.transferSpeed)}/s`}
            icon={GaugeIcon}
          />
          <StatCard 
            title="Time Remaining" 
            value={formatDuration(progress.estimatedTimeRemaining)}
            icon={ClockIcon}
          />
        </div>
      </div>
      
      <div className="batch-progress">
        <h3>Batch Progress</h3>
        <div className="batch-indicator">
          <span>Batch {progress.currentBatch} of {progress.totalBatches}</span>
          <Progress 
            value={(progress.currentBatch / progress.totalBatches) * 100}
            className="batch-progress-bar"
          />
        </div>
      </div>
      
      <div className="file-status-breakdown">
        <div className="status-item success">
          <CheckIcon className="status-icon" />
          <span>{progress.successfulFiles.toLocaleString()} Successful</span>
        </div>
        <div className="status-item error">
          <XIcon className="status-icon" />
          <span>{progress.failedFiles.toLocaleString()} Failed</span>
        </div>
        <div className="status-item skipped">
          <SkipForwardIcon className="status-icon" />
          <span>{progress.skippedFiles.toLocaleString()} Skipped</span>
        </div>
      </div>
    </div>
  )
}
```

**Execution Tasks List** (`components/migration/ExecutionTasks.tsx`)
```tsx
export function ExecutionTasks({ execution }: { execution: ExecutionContext }) {
  const { tasks, isLoading } = useExecutionTasks(execution.executionID)
  const [filterStatus, setFilterStatus] = useState<string>('all')
  
  const filteredTasks = useMemo(() => {
    if (filterStatus === 'all') return tasks
    return tasks?.filter(task => task.status === filterStatus) || []
  }, [tasks, filterStatus])
  
  return (
    <div className="execution-tasks">
      <div className="tasks-header">
        <h3>Migration Tasks</h3>
        <Select value={filterStatus} onValueChange={setFilterStatus}>
          <SelectTrigger className="status-filter">
            <SelectValue placeholder="Filter by status" />
          </SelectTrigger>
          <SelectContent>
            <SelectItem value="all">All Tasks</SelectItem>
            <SelectItem value="pending">Pending</SelectItem>
            <SelectItem value="executing">Running</SelectItem>
            <SelectItem value="complete">Complete</SelectItem>
            <SelectItem value="failed">Failed</SelectItem>
          </SelectContent>
        </Select>
      </div>
      
      {isLoading ? (
        <TasksLoadingSkeleton />
      ) : (
        <div className="tasks-list">
          {filteredTasks.map(task => (
            <ExecutionTaskCard 
              key={task.uid} 
              task={task}
              onRetry={(taskId) => retryTask(taskId)}
              onCancel={(taskId) => cancelTask(taskId)}
            />
          ))}
        </div>
      )}
    </div>
  )
}
```

## Database Schema

**Migration Execution Tables** (`migrations/010_migration_execution.sql`)
```sql
CREATE TABLE migration_executions (
    execution_id TEXT PRIMARY KEY,
    plan_id TEXT NOT NULL REFERENCES migration_plans(id) ON DELETE CASCADE,
    status TEXT NOT NULL CHECK (status IN ('pending', 'running', 'paused', 'completed', 'failed', 'cancelled')),
    started_at INTEGER NOT NULL,
    updated_at INTEGER NOT NULL,
    completed_at INTEGER,
    settings TEXT NOT NULL,     -- JSON ExecutionSettings
    progress TEXT NOT NULL,     -- JSON ExecutionProgress
    errors TEXT NOT NULL,       -- JSON array of ExecutionError
    tasks TEXT NOT NULL,        -- JSON array of task UIDs
    verification_result TEXT    -- JSON VerificationResult, nullable
);

CREATE INDEX idx_migration_executions_plan_id ON migration_executions(plan_id);
CREATE INDEX idx_migration_executions_status ON migration_executions(status);
CREATE INDEX idx_migration_executions_started_at ON migration_executions(started_at);

CREATE TABLE file_operations (
    id TEXT PRIMARY KEY,
    execution_id TEXT NOT NULL REFERENCES migration_executions(execution_id) ON DELETE CASCADE,
    source_path TEXT NOT NULL,
    destination_path TEXT NOT NULL,
    operation TEXT NOT NULL CHECK (operation IN ('copy', 'move', 'hardlink')),
    status TEXT NOT NULL CHECK (status IN ('pending', 'executing', 'completed', 'failed')),
    file_size INTEGER NOT NULL,
    checksum_before TEXT,
    checksum_after TEXT,
    started_at INTEGER,
    completed_at INTEGER,
    error_message TEXT
);

CREATE INDEX idx_file_operations_execution_id ON file_operations(execution_id);
CREATE INDEX idx_file_operations_status ON file_operations(status);
CREATE INDEX idx_file_operations_source_path ON file_operations(source_path);
```

## Validation & Testing

### Execution Engine Tests
- [ ] Plan execution workflow from start to completion
- [ ] Error handling and recovery mechanisms
- [ ] Batch processing and progress tracking
- [ ] Pause, resume, and cancellation functionality
- [ ] Resource management and throttling

### Integration Tests
- [ ] End-to-end migration execution with real files
- [ ] Integration with task engine and organization rules
- [ ] Database operations and state persistence
- [ ] WebSocket real-time updates
- [ ] Verification and validation workflows

### Performance Tests
- [ ] Large file migration performance
- [ ] Concurrent execution handling
- [ ] Memory usage during long migrations
- [ ] I/O optimization and disk utilization
- [ ] Network transfer performance (if applicable)

## Dependencies

**Required Stories:**
- Story 3.1: Migration Planning Foundation
- Story 3.2: Advanced Organization Rules
- Epic 2 Complete: Task Engine, MD5, MediaInfo, UI, Opportunistic Logic
- Story 2.4: Task Queue Management UI (for monitoring integration)

**Technical Dependencies:**
- Task engine for executing migration operations
- Organization rules engine for file structuring
- File system operations and permissions
- Checksum verification capabilities

## Definition of Done

- [ ] Migration execution engine converts plans into executable tasks
- [ ] Safe migration operations with pre-flight validation
- [ ] Real-time progress tracking with detailed statistics
- [ ] Comprehensive error handling with retry and recovery
- [ ] Pause, resume, and cancellation controls
- [ ] Integrity verification using checksums and metadata
- [ ] Batch processing with configurable resource limits
- [ ] Complete API endpoints for execution management
- [ ] Frontend components for execution monitoring
- [ ] Database schema for execution state and history
- [ ] Integration with existing task management UI

## Success Metrics

- **Execution Reliability:** 99.9%+ successful completion rate for valid plans
- **Data Integrity:** 100% integrity verification for all transferred files
- **Performance:** Transfer speeds within 90% of theoretical disk limits
- **Recovery:** 100% successful recovery from interruption or failure
- **User Experience:** Clear progress indication and error reporting

## Next Stories

This story enables:
- **Story 3.4:** Migration Monitoring & Control (enhanced monitoring and intervention)
- **Story 3.5:** Post-Migration Validation (comprehensive validation and reporting)
- **Story 4.1:** External Services Integration (cloud/network migration support)

## PRD Requirements Fulfilled

- **Core Value Proposition:** Safe, reliable data migration execution
- **FR6:** Comprehensive progress tracking and execution monitoring
- **FR7:** Task-based execution system with resource management
- **FR8:** Error handling, retry logic, and recovery mechanisms
- **FR9:** Pause, resume, and cancellation controls
- **FR10:** Real-time progress updates and detailed execution statistics
- **Data Safety:** Integrity verification and atomic operations


## File List

*To be populated during implementation*

## Dev Agent Record

### Implementation Notes
*To be populated during development*

### Completion Notes
*To be populated upon completion*

### Debug Log References
*To be populated if debugging required*

## QA Results

*To be populated during QA review*
